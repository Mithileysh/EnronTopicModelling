This uses the gensim package's implementation of online latent dirichlet allocation to to topic model the enron email
dataset. After training, we can infer a probability distribution over words for each topic in the corpus.

Example topic word distributions:

topic #31: 0.003*market + 0.002*trading + 0.002*risk + 0.002*management + 0.002*million + 0.002*companies + 0.002*services + 0.002*markets 
topic #0: 0.013*intended + 0.011*recipient + 0.006*confidential + 0.005*privileged + 0.005*prohibited + 0.005*attachments + 0.005*sender
topic #3: 0.005*im + 0.005*she + 0.004*her + 0.004*hope + 0.004*night + 0.003*really + 0.003*weekend + 0.003*hey + 0.003*ill + 0.003*love
topic #4: 0.020*sally + 0.012*beck + 0.010*becks + 0.009*sbecknsf + 0.007*sallybeckdec + 0.004*risk + 0.004*operations + 0.004*patti
topic #29: 0.020*image + 0.006*click + 0.005*free + 0.005*order + 0.004*amazoncom + 0.004*online + 0.004*shipping + 0.004*card + 0.003*training
topic #14: 0.014*image + 0.009*travel + 0.006*fares + 0.006*hotel + 0.005*mailbox + 0.004*airlines + 0.004*flight + 0.004*fare + 0.004*mwhitt

We can also infer a probability distribution over topics for each email. Clustering emails by topic can help in information retrieval.
After training, each email is assigned it's most likely topic and stored in mongodb.


Enron emails can be obtained here: http://www.cs.cmu.edu/~enron/

Dependencies:

* gensim - online (memory-independent) topic modeling. Read more here: http://radimrehurek.com/gensim/
* pymongo

